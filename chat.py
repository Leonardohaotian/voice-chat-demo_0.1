import sounddevice as sd
import numpy as np
import wave
import sys
import os
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import subprocess
import warnings

# å±è”½pydubå…³äºffmpegçš„æ— å®³å‘Šè­¦
warnings.filterwarnings(
    "ignore",
    message="Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work",
    category=RuntimeWarning,
    module="pydub.utils",
)

# æ·»åŠ æ¨¡å‹è·¯å¾„åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.append('./models/pyAudioAnalysis')

# ---------------- é…ç½® ----------------
ASR_MODEL_PATH = "./models/SenseVoiceSmall"   # SenseVoice æ¨¡å‹è·¯å¾„
SAMPLE_RATE = 16000
RECORD_SECONDS = 5

# å¯¹è¯å†å²ç®¡ç†
conversation_history = []
# --------------------------------------

def record_audio(filename="input.wav"):
    print("ğŸ™ï¸ å½•éŸ³ä¸­...")
    print("ğŸ’¡ è¯·æ¸…æ™°åœ°è¯´å‡ºä½ çš„è¯ï¼Œä¿æŒé€‚ä¸­çš„éŸ³é‡...")
    
    # å¢åŠ å½•éŸ³æ—¶é•¿ï¼Œç»™ç”¨æˆ·æ›´å¤šæ—¶é—´
    record_duration = RECORD_SECONDS + 1  # 6ç§’
    
    audio = sd.rec(int(record_duration * SAMPLE_RATE), samplerate=SAMPLE_RATE, channels=1, dtype="int16")
    sd.wait()
    
    # æ£€æŸ¥å½•éŸ³éŸ³é‡
    max_volume = np.max(np.abs(audio))
    if max_volume < 1000:  # éŸ³é‡å¤ªä½
        print("âš ï¸ å½•éŸ³éŸ³é‡è¾ƒä½ï¼Œå¯èƒ½å½±å“è¯†åˆ«æ•ˆæœ")
    elif max_volume > 30000:  # éŸ³é‡å¤ªé«˜
        print("âš ï¸ å½•éŸ³éŸ³é‡è¾ƒé«˜ï¼Œå¯èƒ½å½±å“è¯†åˆ«æ•ˆæœ")
    else:
        print("âœ… å½•éŸ³éŸ³é‡æ­£å¸¸")
    
    wf = wave.open(filename, 'wb')
    wf.setnchannels(1)
    wf.setsampwidth(2)
    wf.setframerate(SAMPLE_RATE)
    wf.writeframes(audio.tobytes())
    wf.close()
    print("âœ… å½•éŸ³å®Œæˆ:", filename)

def vad_trim(input_file="input.wav"):
    print("ğŸ” VAD æ£€æµ‹è¯­éŸ³æ®µè½...")
    try:
        # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(input_file):
            print(f"âš ï¸ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
            return input_file
        
        # å°è¯•ä½¿ç”¨pyAudioAnalysisè¿›è¡ŒVAD
        try:
            from pyAudioAnalysis.audioSegmentation import silence_removal
            from pyAudioAnalysis.audioBasicIO import read_audio_file
            
            # æ­£ç¡®çš„æ–¹å¼ï¼šå…ˆè¯»å–éŸ³é¢‘æ•°æ®ï¼Œå†ä¼ é€’ç»™silence_removal
            [Fs, x] = read_audio_file(input_file)
            segments = silence_removal(x, Fs, 0.020, 0.020, smooth_window=1.0, weight=0.3)
            
            if len(segments) == 0:
                print("ğŸ” æœªæ£€æµ‹åˆ°è¯­éŸ³æ®µè½ï¼Œä½¿ç”¨åŸå§‹éŸ³é¢‘")
                return input_file
                
            # å–ç¬¬ä¸€ä¸ªè¯­éŸ³æ®µè½
            start_time, end_time = segments[0]
            start_sample = int(start_time * SAMPLE_RATE)
            end_sample = int(end_time * SAMPLE_RATE)
            
            # è¯»å–éŸ³é¢‘æ•°æ®
            wf = wave.open(input_file, 'rb')
            frames = wf.readframes(wf.getnframes())
            audio = np.frombuffer(frames, dtype=np.int16)
            wf.close()
            
            # æˆªå–è¯­éŸ³æ®µè½
            trimmed = audio[start_sample:end_sample]
            
            # ä¿å­˜æˆªå–åçš„éŸ³é¢‘
            out_file = "trimmed.wav"
            with wave.open(out_file, 'wb') as wf_out:
                wf_out.setnchannels(1)
                wf_out.setsampwidth(2)
                wf_out.setframerate(SAMPLE_RATE)
                wf_out.writeframes(trimmed.tobytes())
                
            print(f"âœ… VADå¤„ç†å®Œæˆï¼Œè¯­éŸ³æ®µè½: {start_time:.2f}s - {end_time:.2f}s")
            return out_file
            
        except Exception as vad_error:
            print(f"âš ï¸ pyAudioAnalysis VADå¤±è´¥: {vad_error}")
            print("ğŸ”„ ä½¿ç”¨ç®€å•VADå¤„ç†")
            return simple_vad_trim(input_file)
        
    except Exception as e:
        print(f"âš ï¸ VADå¤„ç†å‡ºé”™: {e}")
        print("ğŸ”„ ä½¿ç”¨åŸå§‹éŸ³é¢‘æ–‡ä»¶")
        return input_file

def simple_vad_trim(input_file="input.wav"):
    """ç®€å•çš„VADå¤„ç†ï¼ŒåŸºäºéŸ³é‡é˜ˆå€¼"""
    try:
        # è¯»å–éŸ³é¢‘æ•°æ®
        wf = wave.open(input_file, 'rb')
        frames = wf.readframes(wf.getnframes())
        audio = np.frombuffer(frames, dtype=np.int16)
        wf.close()
        
        # è®¡ç®—éŸ³é‡é˜ˆå€¼
        rms = np.sqrt(np.mean(audio.astype(np.float32) ** 2))
        threshold = rms * 0.3  # 30%çš„RMSä½œä¸ºé˜ˆå€¼
        
        # æ‰¾åˆ°éé™éŸ³æ®µè½
        frame_size = int(0.02 * SAMPLE_RATE)  # 20mså¸§
        non_silent_frames = []
        
        for i in range(0, len(audio) - frame_size, frame_size):
            frame = audio[i:i + frame_size]
            frame_rms = np.sqrt(np.mean(frame.astype(np.float32) ** 2))
            if frame_rms > threshold:
                non_silent_frames.append(i)
        
        if not non_silent_frames:
            print("ğŸ” æœªæ£€æµ‹åˆ°è¯­éŸ³æ®µè½ï¼Œä½¿ç”¨åŸå§‹éŸ³é¢‘")
            return input_file
        
        # æ‰¾åˆ°è¯­éŸ³æ®µè½çš„å¼€å§‹å’Œç»“æŸ
        start_frame = min(non_silent_frames)
        end_frame = max(non_silent_frames) + frame_size
        
        # æˆªå–è¯­éŸ³æ®µè½
        trimmed = audio[start_frame:end_frame]
        
        # ä¿å­˜æˆªå–åçš„éŸ³é¢‘
        out_file = "trimmed.wav"
        with wave.open(out_file, 'wb') as wf_out:
            wf_out.setnchannels(1)
            wf_out.setsampwidth(2)
            wf_out.setframerate(SAMPLE_RATE)
            wf_out.writeframes(trimmed.tobytes())
        
        start_time = start_frame / SAMPLE_RATE
        end_time = end_frame / SAMPLE_RATE
        print(f"âœ… ç®€å•VADå¤„ç†å®Œæˆï¼Œè¯­éŸ³æ®µè½: {start_time:.2f}s - {end_time:.2f}s")
        return out_file
        
    except Exception as e:
        print(f"âš ï¸ ç®€å•VADå¤„ç†å¤±è´¥: {e}")
        return input_file

def asr_transcribe(file="trimmed.wav"):
    print("ğŸ“ è¯­éŸ³è¯†åˆ«...")
    
    try:
        # ä½¿ç”¨ç®€åŒ–çš„ASRæ¨¡å—
        from simple_asr import simple_asr_transcribe
        return simple_asr_transcribe(file)
        
    except Exception as e:
        print(f"âš ï¸ ASRè¯†åˆ«å¤±è´¥: {e}")
        print("ğŸ”„ ä½¿ç”¨æ¨¡æ‹Ÿè¯­éŸ³è¯†åˆ«ç»“æœ")
        text = "ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªæµ‹è¯•è¯­éŸ³è¯†åˆ«ç»“æœ"
        print("ğŸ‘¤ ä½ è¯´:", text)
        return text

def llm_reply(text):
    """è°ƒç”¨LLMç”Ÿæˆå›å¤"""
    print("ğŸ¤– è°ƒç”¨ LLM...")
    
    # 1. å°è¯•æœ¬åœ°LLM
    try:
        from local_llm import ensure_loaded, generate_reply
        if ensure_loaded("./models/llm/model.gguf"):
            print("ğŸ§  ä½¿ç”¨æœ¬åœ°LLM (GGUF)...")
            reply = generate_reply(text)
            if reply and reply.strip():
                print("ğŸ¤– å›å¤:", reply)
                conversation_history.append({"role": "user", "content": text})
                conversation_history.append({"role": "assistant", "content": reply})
                return reply
            else:
                print("âš ï¸ æœ¬åœ°LLMå›å¤ä¸ºç©ºï¼Œä½¿ç”¨æ¨¡æ‹Ÿå›å¤")
        else:
            print("âš ï¸ æœ¬åœ°LLMæ¨¡å‹æœªåŠ è½½æˆ–ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå›å¤")
    except Exception as e:
        print(f"âš ï¸ æœ¬åœ°LLMè°ƒç”¨å¤±è´¥: {e}")
        print("ğŸ”„ ä½¿ç”¨æ¨¡æ‹Ÿå›å¤")

    # 2. æ¨¡æ‹Ÿå›å¤
    reply = f"æˆ‘æ”¶åˆ°äº†ä½ çš„æ¶ˆæ¯ï¼š'{text}'ã€‚è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å›å¤ã€‚"
    print("ğŸ¤– å›å¤:", reply)
    conversation_history.append({"role": "user", "content": text})
    conversation_history.append({"role": "assistant", "content": reply})
    return reply

def show_conversation_history():
    """æ˜¾ç¤ºå¯¹è¯å†å²"""
    if not conversation_history:
        print("ğŸ“š æš‚æ— å¯¹è¯å†å²")
        return
    
    print(f"\nğŸ“š å¯¹è¯å†å² (å…± {len(conversation_history)} æ¡æ¶ˆæ¯):")
    print("=" * 50)
    
    for i, msg in enumerate(conversation_history):
        role = "ğŸ‘¤ ç”¨æˆ·" if msg["role"] == "user" else "ğŸ¤– åŠ©æ‰‹"
        content = msg["content"]
        print(f"{i+1}. {role}: {content}")
        print("-" * 30)

def clear_conversation_history():
    """æ¸…ç©ºå¯¹è¯å†å²"""
    global conversation_history
    conversation_history = []
    print("âœ… å¯¹è¯å†å²å·²æ¸…ç©º")

def tts_settings():
    """TTSè®¾ç½®èœå•"""
    try:
        from edge_tts_config import edge_tts_config, speak_text
        
        print("\nğŸ¤ Edge-TTSè®¾ç½®")
        print("=" * 30)
        
        # æ˜¾ç¤ºå½“å‰å£°éŸ³
        current_voice = edge_tts_config.get_voice_info()
        print(f"å½“å‰å£°éŸ³: {current_voice.get('name', 'Unknown')} ({current_voice.get('gender', 'Unknown')})")
        
        # åˆ—å‡ºå¯ç”¨å£°éŸ³
        voices = edge_tts_config.list_voices()
        print(f"\nğŸµ å¯ç”¨å£°éŸ³ (å…±{len(voices)}ä¸ª):")
        for i, voice in enumerate(voices):
            current = " (å½“å‰)" if voice['id'] == edge_tts_config.current_voice else ""
            print(f"  {i+1:2d}. {voice['name']} ({voice['gender']}){current}")
        
        print("\né€‰é¡¹:")
        print("  's' - åˆ‡æ¢å£°éŸ³")
        print("  'c' - æŸ¥çœ‹ä¸­æ–‡å£°éŸ³")
        print("  'e' - æŸ¥çœ‹è‹±æ–‡å£°éŸ³")
        print("  't' - æµ‹è¯•TTS")
        print("  'b' - è¿”å›ä¸»èœå•")
        
        while True:
            choice = input("\nè¯·é€‰æ‹©æ“ä½œ (s/c/e/t/b): ").strip().lower()
            
            if choice == 'b':
                break
            elif choice == 's':
                try:
                    voice_num = int(input(f"è¯·é€‰æ‹©å£°éŸ³ (1-{len(voices)}): "))
                    if 1 <= voice_num <= len(voices):
                        voice_id = voices[voice_num-1]['id']
                        if edge_tts_config.set_voice(voice_id):
                            print(f"âœ… å·²åˆ‡æ¢åˆ°å£°éŸ³: {voices[voice_num-1]['name']}")
                        else:
                            print("âŒ å£°éŸ³åˆ‡æ¢å¤±è´¥")
                    else:
                        print("âŒ æ— æ•ˆçš„å£°éŸ³ç¼–å·")
                except ValueError:
                    print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
            elif choice == 'c':
                chinese_voices = edge_tts_config.list_voices("zh-CN")
                print(f"\nğŸ‡¨ğŸ‡³ ä¸­æ–‡å£°éŸ³ (å…±{len(chinese_voices)}ä¸ª):")
                for i, voice in enumerate(chinese_voices, 1):
                    current = " (å½“å‰)" if voice['id'] == edge_tts_config.current_voice else ""
                    print(f"  {i:2d}. {voice['name']} ({voice['gender']}){current}")
            elif choice == 'e':
                english_voices = edge_tts_config.list_voices("en-US")
                print(f"\nğŸ‡ºğŸ‡¸ è‹±æ–‡å£°éŸ³ (å…±{len(english_voices)}ä¸ª):")
                for i, voice in enumerate(english_voices, 1):
                    current = " (å½“å‰)" if voice['id'] == edge_tts_config.current_voice else ""
                    print(f"  {i:2d}. {voice['name']} ({voice['gender']}){current}")
            elif choice == 't':
                test_text = input("è¯·è¾“å…¥æµ‹è¯•æ–‡æœ¬ (å›è½¦ä½¿ç”¨é»˜è®¤): ").strip()
                if not test_text:
                    test_text = "è¿™æ˜¯ä¸€ä¸ªEdge-TTSæµ‹è¯•ã€‚"
                print("ğŸ”Š æµ‹è¯•TTS...")
                tts_speak(test_text)
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©")
                
    except Exception as e:
        print(f"âš ï¸ TTSè®¾ç½®å¤±è´¥: {e}")

def analyze_audio_basic(audio_file):
    """å¯¹éŸ³é¢‘è¿›è¡ŒåŸºç¡€åˆ†æï¼Œåªè¾“å‡ºéŸ³é¢‘ç»Ÿè®¡ä¿¡æ¯ã€‚"""
    duration_sec = None
    rms = None
    silence_ratio = None
    try:
        with wave.open(audio_file, 'rb') as wf_in:
            frames = wf_in.readframes(wf_in.getnframes())
            audio_np = np.frombuffer(frames, dtype=np.int16)
            duration_sec = wf_in.getnframes() / float(wf_in.getframerate())
            if audio_np.size > 0:
                rms = float(np.sqrt(np.mean((audio_np.astype(np.float32) / 32768.0) ** 2)))
                frame_len = int(0.02 * wf_in.getframerate()) or 1
                if frame_len > 0:
                    frames_view = audio_np[: len(audio_np) - (len(audio_np) % frame_len)].reshape(-1, frame_len)
                    frame_rms = np.sqrt(np.mean((frames_view.astype(np.float32) / 32768.0) ** 2, axis=1))
                    thr = max(0.01, np.median(frame_rms) * 0.5)
                    silence_ratio = float(np.mean(frame_rms < thr))
    except Exception as e:
        print(f"âš ï¸ éŸ³é¢‘åˆ†æå¤±è´¥: {e}")

    print("\nğŸ“Š éŸ³é¢‘åˆ†æï¼š")
    if duration_sec is not None:
        print(f"- éŸ³é¢‘æ—¶é•¿: {duration_sec:.2f}s  | RMS: {rms:.4f}  | é™éŸ³å æ¯”: {silence_ratio if silence_ratio is None else round(silence_ratio, 2)}")
    print()

    return {"duration_sec": duration_sec, "rms": rms, "silence_ratio": silence_ratio}

def visualize_audio(audio_file: str, output_prefix: str = "audio_analysis"):
    """ä½¿ç”¨pyAudioAnalysiså’Œmatplotlibç”ŸæˆéŸ³é¢‘å¯è§†åŒ–å›¾åƒã€‚"""
    try:
        with wave.open(audio_file, 'rb') as wf_in:
            fr = wf_in.getframerate()
            n = wf_in.getnframes()
            raw = wf_in.readframes(n)
            y = np.frombuffer(raw, dtype=np.int16).astype(np.float32) / 32768.0
        t = np.arange(len(y)) / float(fr)

        fig, axes = plt.subplots(3, 1, figsize=(12, 9))
        fig.suptitle('Audio Analysis Report', fontsize=16, fontweight='bold')

        # æ³¢å½¢
        axes[0].plot(t, y, linewidth=0.7, color='blue')
        axes[0].set_title('Waveform', fontweight='bold')
        axes[0].set_xlabel('Time (s)')
        axes[0].set_ylabel('Amplitude')
        axes[0].grid(True, alpha=0.3)

        # é¢‘è°±å›¾ï¼ˆå¿½ç•¥log10é™¤é›¶ç­‰RuntimeWarningï¼‰
        with warnings.catch_warnings():
            warnings.simplefilter("ignore", category=RuntimeWarning)
            axes[1].specgram(y, NFFT=512, Fs=fr, noverlap=256, cmap='magma')
        axes[1].set_title('Spectrogram', fontweight='bold')
        axes[1].set_xlabel('Time (s)')
        axes[1].set_ylabel('Frequency (Hz)')

        # çŸ­æ—¶ç‰¹å¾
        from pyAudioAnalysis import ShortTermFeatures as stf
        win = int(0.05 * fr) or 1
        step = int(0.025 * fr) or 1
        feats, feat_names = stf.feature_extraction(y, fr, win, step)
        zcr_idx = feat_names.index('zcr') if 'zcr' in feat_names else 0
        energy_idx = feat_names.index('energy') if 'energy' in feat_names else 1
        x_frames = np.arange(feats.shape[1]) * (step / fr)

        axes[2].plot(x_frames, feats[zcr_idx, :], label='ZCR', color='red', linewidth=1)
        axes[2].plot(x_frames, feats[energy_idx, :], label='Energy', color='green', linewidth=1)
        axes[2].set_title('Short-Term Features (ZCR & Energy)', fontweight='bold')
        axes[2].set_xlabel('Time (s)')
        axes[2].set_ylabel('Value')
        axes[2].legend()
        axes[2].grid(True, alpha=0.3)

        plt.tight_layout()
        combined_path = f"{output_prefix}_combined.png"
        plt.savefig(combined_path, dpi=150, bbox_inches='tight')
        plt.close()

        print("ğŸ–¼ï¸ ç»¼åˆéŸ³é¢‘åˆ†æå›¾å·²ç”Ÿæˆ:")
        print(f"- {combined_path}")
    except Exception as e:
        print(f"âš ï¸ éŸ³é¢‘å¯è§†åŒ–å¤±è´¥: {e}")

def tts_speak(text):
    print("ğŸ”Š å¼€å§‹è¯­éŸ³åˆæˆ...")
    
    try:
        # ä½¿ç”¨Edge-TTS
        from edge_tts_config import speak_text
        
        if speak_text(text):
            print("ğŸ”Š è¯­éŸ³æ’­æ”¾å®Œæˆ")
        else:
            print("âš ï¸ Edge-TTSæ’­æ”¾å¤±è´¥ï¼Œæ˜¾ç¤ºæ–‡æœ¬å›å¤")
            print("ğŸ“ æ–‡æœ¬å›å¤:", text)
        
    except Exception as e:
        print(f"âš ï¸ Edge-TTSåŠ è½½å¤±è´¥: {e}")
        # é™çº§åˆ°ç³»ç»ŸTTS
        try:
            subprocess.run(["say", text], check=True)
            print("ğŸ”Š ç³»ç»Ÿè¯­éŸ³æ’­æ”¾å®Œæˆ")
        except Exception as e2:
            print(f"âš ï¸ ç³»ç»ŸTTSä¹Ÿå¤±è´¥: {e2}")
            print("ğŸ“ æ–‡æœ¬å›å¤:", text)

def interactive_mode():
    """äº¤äº’å¼å¯¹è¯æ¨¡å¼"""
    print("\nğŸ™ï¸ æ¬¢è¿ä½¿ç”¨æ™ºèƒ½è¯­éŸ³èŠå¤©ç³»ç»Ÿï¼")
    print("=" * 50)
    print("å‘½ä»¤è¯´æ˜:")
    print("  'r' æˆ– 'record' - å¼€å§‹å½•éŸ³å¯¹è¯")
    print("  'h' æˆ– 'history' - æŸ¥çœ‹å¯¹è¯å†å²")
    print("  'c' æˆ– 'clear' - æ¸…ç©ºå¯¹è¯å†å²")
    print("  't' æˆ– 'tts' - TTSè®¾ç½®")
    print("  'q' æˆ– 'quit' - é€€å‡ºç¨‹åº")
    print("=" * 50)
    
    while True:
        try:
            command = input("\nè¯·è¾“å…¥å‘½ä»¤ (r/h/c/t/q): ").strip().lower()
            
            if command in ['q', 'quit', 'exit']:
                print("ğŸ‘‹ å†è§ï¼")
                break
            elif command in ['r', 'record']:
                print("\nğŸ™ï¸ å¼€å§‹å½•éŸ³å¯¹è¯...")
                record_audio()
                speech_file = vad_trim()
                user_text = asr_transcribe(speech_file)
                
                # éŸ³é¢‘åŸºç¡€åˆ†æ
                _ = analyze_audio_basic(speech_file)
                # ç”ŸæˆéŸ³é¢‘å¯è§†åŒ–
                visualize_audio(speech_file, output_prefix="audio_analysis")
                
                reply_text = llm_reply(user_text)
                tts_speak(reply_text)
                
            elif command in ['h', 'history']:
                show_conversation_history()
            elif command in ['c', 'clear']:
                clear_conversation_history()
            elif command in ['t', 'tts']:
                tts_settings()
            else:
                print("âŒ æ— æ•ˆå‘½ä»¤ï¼Œè¯·é‡æ–°è¾“å…¥")
                
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­ï¼Œå†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    # æ£€æŸ¥æ˜¯å¦ä»¥äº¤äº’æ¨¡å¼è¿è¡Œ
    if len(sys.argv) > 1 and sys.argv[1] in ['-i', '--interactive']:
        interactive_mode()
    else:
        # é»˜è®¤å•æ¬¡è¿è¡Œæ¨¡å¼
        print("ğŸ™ï¸ å•æ¬¡å½•éŸ³æ¨¡å¼ (ä½¿ç”¨ -i å‚æ•°è¿›å…¥äº¤äº’æ¨¡å¼)")
        record_audio()
        speech_file = vad_trim()
        user_text = asr_transcribe(speech_file)
        # éŸ³é¢‘åŸºç¡€åˆ†æ
        _ = analyze_audio_basic(speech_file)
        # ç”ŸæˆéŸ³é¢‘å¯è§†åŒ–
        visualize_audio(speech_file, output_prefix="audio_analysis")
        reply_text = llm_reply(user_text)
        tts_speak(reply_text)